{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info about the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset\n",
    "# replace data_file_name with your name of the raw data json file\n",
    "\n",
    "with open(\"data_file_name.json\") as jsonData:\n",
    "    dataset = json.load(jsonData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset with meta data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaData = {}\n",
    "\n",
    "# loop over each participant in the dataset file\n",
    "for participant in dataset:\n",
    "\n",
    "    metaData[participant] = {}\n",
    "\n",
    "    ignore = [\"taskOrder\", \"condition\", \"initScreenProps\"]\n",
    "\n",
    "    for i in dataset[participant][\"ExperimentMetaData\"]:\n",
    "        if i not in ignore and \"failsMediaCheck\" not in i:\n",
    "            metaData[participant][i] = dataset[participant][\"ExperimentMetaData\"][i]\n",
    "\n",
    "df = pd.DataFrame(metaData).T\n",
    "\n",
    "# create a \"complete study completed column\"\n",
    "df.loc[df[\"hasCompleted\"] == True, \"studyCompleted\"] = True\n",
    "df.loc[df[\"lastCompletedPage\"] == \"DonationOption\", \"studyCompleted\"] = True\n",
    "df.loc[df[\"lastCompletedPage\"] == \"Con_Mdbf\", \"studyCompleted\"] = True\n",
    "df[\"studyCompleted\"].fillna(False, inplace=True)\n",
    "\n",
    "# show the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log some metadata information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of study accesses\n",
    "tot_study_open = len(df)\n",
    "# total number of study accesses without a valid panel ID\n",
    "no_panel_id = len(df[df[\"panelId\"] == \"undefined\"])\n",
    "# total number of study accesses with a valid panel ID\n",
    "valid_panel_id = tot_study_open - no_panel_id\n",
    "# internet explorer openings\n",
    "used_ie = len(df[df[\"isInternetExplorer\"]])\n",
    "# edge openings\n",
    "used_edge = len(df[df[\"isEdge\"]])\n",
    "# completed study with edge\n",
    "edge_compl = len(df[df[\"isEdge\"] & df[\"studyCompleted\"]])\n",
    "# fails the initial media check (touch or screen too small)\n",
    "fails_init_media_check = len(df[df[\"failsInitialMediaCheck\"] == True])\n",
    "# answer that no mouse is used in the study\n",
    "no_mouse = fails_init_media_check = len(df[df[\"hasNoMouse\"] == True])\n",
    "\n",
    "# completion criterion\n",
    "# has_completed\n",
    "all_comp = len(df[df[\"hasCompleted\"] == True])\n",
    "# last page Donation Option\n",
    "read_debriefing = len(df[df[\"lastCompletedPage\"] == \"DonationOption\"])\n",
    "# last page Condition MDBF\n",
    "completed_mdbf = len(df[df[\"lastCompletedPage\"] == \"Con_Mdbf\"])\n",
    "# completed the study\n",
    "completed = all_comp + read_debriefing + completed_mdbf\n",
    "\n",
    "# print some information about the dataset\n",
    "print(\"Total number of study opens: \", tot_study_open)\n",
    "print(\"Study opens without a panel Id: \", no_panel_id)\n",
    "print(\"Study opens with a panel Id: \", valid_panel_id)\n",
    "print(\"Used Internet Explorer to open the study: \", used_ie)\n",
    "print(\"Used Edge to open the study: \", used_edge)\n",
    "print(\"Finished the study with Edge: \", edge_compl)\n",
    "print(\"Fails initial Media check: \", fails_init_media_check)\n",
    "print(\"Reports using no mouse: \", no_mouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset with unique study finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the number of times persons with the same panel id participated\n",
    "panel_ids = df[\"panelId\"].value_counts()\n",
    "repetitive_panel_ids = panel_ids[panel_ids > 1]\n",
    "print(\"Number of Participants who opened the study more than once: \", len(repetitive_panel_ids))\n",
    "\n",
    "# create a df that only has unique panel ids\n",
    "cleaned_df = df\n",
    "\n",
    "# loop over the repetive panel Ids\n",
    "for panel_id in repetitive_panel_ids.index:\n",
    "\n",
    "    # get the data of the person that opened the study more than once\n",
    "    pers = cleaned_df[cleaned_df[\"panelId\"] == panel_id]\n",
    "\n",
    "    # get the index of the completed trials\n",
    "    index_to_keep = pers[pers[\"studyCompleted\"] == True].index\n",
    "    # check is the same person completed the study more than once\n",
    "    have_completed = pers[pers[\"studyCompleted\"] == True]\n",
    "    # if the study was completed more than once by the same person, only keep the first data of the first completion\n",
    "    if len(have_completed) > 1:\n",
    "        print(\"Participant with panel id \" + panel_id + \" completed the study \" + str(len(have_completed)) + \" times\")\n",
    "        # only keep the first completed trial\n",
    "        index_to_keep = pd.to_numeric(have_completed[\"startTime\"]).idxmin()\n",
    "\n",
    "    # if the study was completed, keep the data of the completed trial\n",
    "    if len(index_to_keep) > 0:\n",
    "        ind_to_del = pers.index.drop(index_to_keep)\n",
    "    # if the study was not completed, keep a random data file\n",
    "    else:\n",
    "        ind_to_del = pers.index[:-1]\n",
    "\n",
    "    cleaned_df = cleaned_df.drop(ind_to_del)\n",
    "\n",
    "# check if repetivive ids have been successfully removed\n",
    "new_panel_ids = cleaned_df[\"panelId\"].value_counts()\n",
    "new_repetitive_panel_ids = new_panel_ids[new_panel_ids > 1]\n",
    "print(\"Number of Participants who opened the study more than once after cleanup: \", len(new_repetitive_panel_ids))\n",
    "\n",
    "# remove the \"undefined\"\n",
    "cleaned_df = cleaned_df.drop(cleaned_df[cleaned_df[\"panelId\"] == \"undefined\"].index)\n",
    "\n",
    "print(\"Unique study access: \", len(cleaned_df))\n",
    "print(\"Number of study finishes: \", len(cleaned_df[cleaned_df[\"studyCompleted\"]]))\n",
    "\n",
    "# save the index values (firebase ids) of the completed studies\n",
    "ids_completed = cleaned_df[cleaned_df[\"studyCompleted\"]].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset with the study durations of each study part as well as the total study duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_durations = {}\n",
    "\n",
    "# get the duration of the actual task in the practice condition\n",
    "pr_tasks = [\"Pr_DragDrop\", \"Pr_FollowBox\", \"Pr_PatternTyping\", \"Pr_PointClick\", \"Pr_Slider\"]\n",
    "\n",
    "# loop over all datasets\n",
    "for par in dataset:\n",
    "    \n",
    "    # only include the data of the finished studies\n",
    "    if par in ids_completed:\n",
    "\n",
    "        study_durations[par] = {}\n",
    "\n",
    "        # get info about the page duration and the study duration\n",
    "        study_duration = 0\n",
    "        condition_duration = 0\n",
    "\n",
    "        # loop over all study pages and save the page duration time plus add it to the total time\n",
    "        for study_page in dataset[par]:\n",
    "\n",
    "            if \"MetaData\" in dataset[par][study_page]:\n",
    "                study_durations[par][study_page + \"_duration\"] = dataset[par][study_page][\"MetaData\"][\n",
    "                                                                       \"pageDuration\"] / 1000\n",
    "                study_duration += dataset[par][study_page][\"MetaData\"][\"pageDuration\"]\n",
    "\n",
    "                # if its a mouse practice task, additionally get the time of the actual task\n",
    "                if study_page in pr_tasks:\n",
    "                    study_durations[par][study_page + \"_Task_duration\"] = (dataset[par][study_page][\"data\"][\n",
    "                                                                         \"taskEnded\"] - dataset[par][study_page][\"data\"][\n",
    "                                                                         \"trialStarted\"]) / 1000\n",
    "                # if its a page from the actual condition, add its time to the condition duration (except for the intro\n",
    "                # page)\n",
    "                if \"Con_\" in study_page and study_page != \"Con_Instr\":\n",
    "                    condition_duration += dataset[par][study_page][\"MetaData\"][\"pageDuration\"]\n",
    "\n",
    "\n",
    "        # save total study duration in minutes\n",
    "        study_durations[par][\"study_duration\"] = study_duration / 1000 / 60\n",
    "        # save the condition duration\n",
    "        study_durations[par][\"condition_duration\"] = condition_duration / 1000 / 60\n",
    "\n",
    "study_durations_df = pd.DataFrame(study_durations).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier and bad case removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing based bad case detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag cases that:\n",
    "\n",
    "# - had a total study duration shorter than a possible time for actual participation (personal time to click through\n",
    "# the study without reading anything and with knowledge about the study was about 12 minutes)\n",
    "\n",
    "short_study_duration = study_durations_df.loc[study_durations_df[\"study_duration\"] < 12]\n",
    "print(\"Short study duration outliers\", len(short_study_duration))\n",
    "\n",
    "# - had a too short duration to fill out the mdbf (in personal tests, clicking through the mdbf without reading\n",
    "# took about 13-15 seconds), filling the questionnaire out as fast as possible with \"correct\" answers took about 20\n",
    "# seconds (with good knowledge about the questions)\n",
    "\n",
    "# the mdbf in the practice condition should take longer than the mdbf in the actual condition because it is new\n",
    "short_mdbf_pr = study_durations_df.loc[study_durations_df[\"Pr_Mdbf_duration\"] < 21]\n",
    "short_mdbf_co = study_durations_df.loc[study_durations_df[\"Con_Mdbf_duration\"] < 18]\n",
    "print(\"Co_MDBF duration outliers condition\", len(short_mdbf_co),\n",
    "      \"\\n\" + \"Pr_MDBF duration outliers practice\", len(short_mdbf_pr))\n",
    "\n",
    "# we removed participants if their count task on average took longer than 65 seconds(either because of technical difficulties or\n",
    "# because the task was interrupted by changing browser tabs etc... (-> the manipulation did not work as intended!)\n",
    "# the animation of the loading bar had problems \"loading\" in some cases, which caused the loading task length to be\n",
    "# pretty high\n",
    "# the expected duration of the count task without leaving the experimental page or technical difficulties was 45 seconds\n",
    "# we therefore added a 20 second puffer before flagging the participant dataset as an outlier\n",
    "\n",
    "# This was included because technical difficulties (or pauses during the task) likely changed the outcome of the\n",
    "# manipulation in a not desired direction --> technical difficulties cause frustration and stress independent of the\n",
    "# condition\n",
    "\n",
    "study_durations_df[\"avg_count_task_duration\"] = study_durations_df.loc[:, [\"Con_Count_DragDrop_duration\",\n",
    "                                                                           \"Con_Count_FollowBox_duration\",\n",
    "                                                                           \"Con_Count_PatternTyping_duration\",\n",
    "                                                                           \"Con_Count_PointClick_duration\",\n",
    "                                                                           \"Con_Count_Slider_duration\"]].mean(axis=1)\n",
    "\n",
    "bad_count_duration = study_durations_df.loc[study_durations_df[\"avg_count_task_duration\"] > 65]\n",
    "\n",
    "# Add the time based outliers together\n",
    "\n",
    "timebased_outliers = list(short_study_duration.index) + list(short_mdbf_pr.index) + list(short_mdbf_co.index) + \\\n",
    "                     list(bad_count_duration.index)\n",
    "\n",
    "print(\"Total time based outliers\", len(list(set(timebased_outliers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer based bad case detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cases without variance in the mdbf questionnaires (participants always clicked on the same answer in the MDBF)\n",
    "\n",
    "# create a df with the mdbf answers\n",
    "mdbf_answers = {}\n",
    "\n",
    "for par in dataset:\n",
    "    # only include the data of the finished studies\n",
    "    if par in ids_completed:\n",
    "\n",
    "        mdbf_answers[par] = {}\n",
    "\n",
    "        # Condition\n",
    "        for item in dataset[par][\"Con_Mdbf\"][\"data\"]:\n",
    "            value = dataset[par][\"Con_Mdbf\"][\"data\"][item]\n",
    "            mdbf_answers[par][\"Con_\" + item] = value\n",
    "\n",
    "        # Practice\n",
    "        for item in dataset[par][\"Pr_Mdbf\"][\"data\"]:\n",
    "            value = dataset[par][\"Pr_Mdbf\"][\"data\"][item]\n",
    "            mdbf_answers[par][\"Pr_\" + item] = value\n",
    "\n",
    "mdbf_answers_df = pd.DataFrame(mdbf_answers).T\n",
    "\n",
    "# filter the practice and actual condition, calculate the standard deviation of the mdbf answers and get the cases\n",
    "# that have no standard deviation (= answered all questions equally)\n",
    "practice_mdfb = mdbf_answers_df.filter(regex=\"Pr\")\n",
    "practice_mdfb = practice_mdfb.assign(std=practice_mdfb.std(axis=1))\n",
    "no_variance_pr = practice_mdfb.loc[practice_mdfb[\"std\"] == 0]\n",
    "\n",
    "print(\"No variance in the practice mdbf\", len(no_variance_pr))\n",
    "\n",
    "condition_mdfb = mdbf_answers_df.filter(regex=\"Con\")\n",
    "condition_mdfb = condition_mdfb.assign(std=condition_mdfb.std(axis=1))\n",
    "no_variance_con = condition_mdfb.loc[condition_mdfb[\"std\"] == 0]\n",
    "\n",
    "print(\"No variance in the practice mdbf\", len(no_variance_con))\n",
    "\n",
    "bad_mdbf_answers = list(no_variance_pr.index) + list(no_variance_con.index)\n",
    "# filter double cases\n",
    "bad_mdbf_answers = list(set(bad_mdbf_answers))\n",
    "print(\"Total number of bad mdbf answer cases\", len(bad_mdbf_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cases with bad answer behavior in the count task (did not give a proper answer to the count task (indication\n",
    "# that the task and therefore the stress manipulation was not done properly)\n",
    "\n",
    "# get the count task answers\n",
    "\n",
    "count_tasks = ['Con_CountAns_DragDrop',\n",
    "               'Con_CountAns_FollowBox',\n",
    "               'Con_CountAns_PatternTyping',\n",
    "               'Con_CountAns_PointClick',\n",
    "               'Con_CountAns_Slider']\n",
    "\n",
    "count_answers_data = {}\n",
    "\n",
    "for par in dataset:\n",
    "\n",
    "    if par in ids_completed:\n",
    "\n",
    "        count_answers_data[par] = {}\n",
    "\n",
    "        # get condition\n",
    "        count_answers_data[par][\"condition\"] = dataset[par][\"ExperimentMetaData\"][\"condition\"]\n",
    "\n",
    "        for i in count_tasks:\n",
    "            # get the string of the count task (to get the correct target number)\n",
    "            k = i.replace(\"CountAns\", \"Count\")\n",
    "\n",
    "            task_string = i[13:]\n",
    "\n",
    "            count_task_answer = dataset[par][i][\"data\"][\"Count_Task_Answer\"]\n",
    "            count_task_solution = dataset[par][k][\"data\"][\"Total_num_targets\"]\n",
    "\n",
    "            # get the answer and solution of the count task aswell as the difference between the answer and solution\n",
    "            # per task\n",
    "\n",
    "            count_answers_data[par][task_string + \"_CountSol\"] = count_task_solution\n",
    "            count_answers_data[par][task_string + \"_CountAns\"] = count_task_answer\n",
    "            count_answers_data[par][task_string + \"Difference\"] = abs(count_task_solution - count_task_answer)\n",
    "\n",
    "count_df = pd.DataFrame(count_answers_data).T\n",
    "\n",
    "# add columns about the total count task results (total solution, total answer, total difference between all\n",
    "# answers and all solutions aswell as the difference between the final answer and the final solution\n",
    "count_df[\"total_solution\"] = count_df.loc[: , count_df.columns.str.contains(\"_CountSol\")].sum(axis=1)\n",
    "count_df[\"total_answer\"] = count_df.loc[: , count_df.columns.str.contains(\"_CountAns\")].sum(axis=1)\n",
    "count_df[\"total_difference\"] = count_df.loc[: , count_df.columns.str.contains(\"Difference\")].sum(axis=1)\n",
    "count_df[\"result_difference\"] = abs(count_df[\"total_solution\"] - count_df[\"total_answer\"])\n",
    "\n",
    "# separate the high stress and low stress condition\n",
    "hs_count_df = count_df.loc[count_df[\"condition\"] == 0]\n",
    "ls_count_df = count_df.loc[count_df[\"condition\"] == 1]\n",
    "\n",
    "\n",
    "# get cases whose answer to the count task deviates more than 60 % from the solution (= gave no answer in 3 of the 5\n",
    "# count tasks)\n",
    "bad_count_hs = hs_count_df.loc[hs_count_df[\"total_difference\"] >= (hs_count_df[\"total_solution\"] * 0.6)]\n",
    "# print(np.median(hs_count_df[\"total_solution\"] * 0.6))\n",
    "bad_count_ls = ls_count_df.loc[ls_count_df[\"total_difference\"] >= (ls_count_df[\"total_solution\"] * 0.6)]\n",
    "# print(np.median(ls_count_df[\"total_solution\"] * 0.6))\n",
    "\n",
    "# get cases that always give the same answers\n",
    "hs_count_answers = hs_count_df.filter(regex=\"CountAns\")\n",
    "hs_count_answers = hs_count_answers.assign(std=hs_count_answers.std(axis=1))\n",
    "no_variance_count_hs = hs_count_answers.loc[hs_count_answers[\"std\"] == 0]\n",
    "\n",
    "ls_count_answers = bad_count_ls.filter(regex=\"CountAns\")\n",
    "ls_count_answers = ls_count_answers.assign(std=ls_count_answers.std(axis=1))\n",
    "no_variance_count_ls = ls_count_answers.loc[ls_count_answers[\"std\"] == 0]\n",
    "\n",
    "# visual inspection of the data in the high-stress condition revelead that one person most likely did a typo in one of his\n",
    "# answers and two participants continously counted instead of counting per task (might include them in the analysis)\n",
    "# ['3VyP4jiey9RwXCRTKpa7uOXPRU42', '7W9ozOPxiMSZ9tKA4YwSBx5XWb73', 'IOvNKGD2hEZI96zr0j2mFO30Dct1']\n",
    "# similarily in the low stress condition, two participants most likely continously counted instead of per task\n",
    "# ['L6AXSwn87lNO34kqlP8VZcXvaG33', 'LUJKIt6thTf07ywR4KCUwn8mK7X2']\n",
    "self_evaluated_okay_cases = [\"3VyP4jiey9RwXCRTKpa7uOXPRU42\", \"7W9ozOPxiMSZ9tKA4YwSBx5XWb73\",\n",
    "                             \"IOvNKGD2hEZI96zr0j2mFO30Dct1\", \"L6AXSwn87lNO34kqlP8VZcXvaG33\",\n",
    "                             \"LUJKIt6thTf07ywR4KCUwn8mK7X2\"]\n",
    "\n",
    "print(\"Bad cases in the high stress count task\", len(bad_count_hs) - 3)\n",
    "print(\"Bad cases in the low stress count task\", len(bad_count_ls) - 2)\n",
    "\n",
    "bad_cases_count = list(bad_count_hs.index) + list(bad_count_ls.index) + list(no_variance_count_hs.index) + \\\n",
    "                  list(no_variance_count_ls.index)\n",
    "bad_cases_count = list(set(bad_cases_count))\n",
    "bad_cases_count = [i for i in bad_cases_count if i not in self_evaluated_okay_cases]\n",
    "print(\"Total bad cases in the count task\", len(bad_cases_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add all bad cases together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all possible bad datapoints based on the different analysis\n",
    "total_bad_cases = timebased_outliers + bad_mdbf_answers + bad_cases_count\n",
    "# filter duplicate cases\n",
    "total_bad_cases = list(set(total_bad_cases))\n",
    "\n",
    "print(\"Total bad cases identified with the previous steps\", len(total_bad_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some info about the study duration of the dataset after removal of the outliers\n",
    "get the study duration\n",
    "new_study_durations = study_durations_df.drop(total_bad_cases)\n",
    "\n",
    "print(\"Median Study duration:\", np.median(new_study_durations[\"study_duration\"]))\n",
    "print(\"Standard Deviation of Study duration:\", np.std(new_study_durations[\"study_duration\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the index values of all participants who completed the study and the index values of the participants who\n",
    "# completed the study after removing the bad cases\n",
    "\n",
    "# all study ids\n",
    "with open(\"all_study_finished\", \"wb\") as fp:\n",
    "    pickle.dump(ids_completed, fp)\n",
    "\n",
    "# study ids after removal of bad cases\n",
    "with open(\"study_ids_without_bad_cases\", \"wb\") as fp:\n",
    "    pickle.dump(set(ids_completed) - set(total_bad_cases), fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full_conda]",
   "language": "python",
   "name": "conda-env-full_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
